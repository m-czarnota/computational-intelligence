{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2732c90c-b159-4783-81f3-26fa4b8e8f1c",
   "metadata": {},
   "source": [
    "Zadanie polegało na przeprowadzeniu zbiorowego eksperymentu z wykorzystaniem klasyfikatorów liniowych. Dla każdego z nich należało przetestować te same dane po podziale na zadany rozmiar części testowej.\n",
    "Wyniki z dla poszczególnych zbiorów danych zostały przedstawione w strukturze `DataFrame` oraz `pivot_table`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ca53a-4939-4d64-ac77-fadd54e409f5",
   "metadata": {},
   "source": [
    "Importy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8228f67-0452-40cb-99f5-5c4322fd63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import fetch_rcv1, fetch_openml\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823ee96f-528b-4844-83b4-9b2090b2f40a",
   "metadata": {},
   "source": [
    "Klasa służąca do wykonywania testów związanych z liniowymi klasyfikatorami.\n",
    "\n",
    "Opis parametrów:\n",
    "* `max_iter` - maksymalna liczba iteracji dla wykorzystanych klasyfikatorów\n",
    "* `test_sizes` - tablica zawierająca rozmiary części uczącej po podziale danych\n",
    "* `regularization_params` - tablica zawierająca listę parametrów regularyzacyjnych do przetestowania - wykorzystywana w `GridSearchCV`\n",
    "* `results_count_per_test_size` - liczba iteracji do wykonania na każdy z rozmiarów testowych. Wykorzystywane do wielokrotnego podziału tych samych danych, a następnie uśredniania zebranych wyników\n",
    "* `classifiers` - klasyfikatory wykorzystane w eksperymencie:\n",
    "    * `SVC` jako wersja liniowa\n",
    "    * `MLPClassifier` jako wersja liniowa bez warstw ukrytych\n",
    "    * `LogisticRegression` z funkcjami kary `L1` oraz `L2`\n",
    "    \n",
    "Własna implementacja `SVM2` nie została wykorzystana ze względu na niezidentyfikowane problemy z działaniem biblioteki `cvxopt`.\n",
    "\n",
    "Opis metod:\n",
    "* `experiment` - główna metoda wykonująca eksperyment na wszytkich zbiorach danych, zbierająca wyniki z każdego zbioru danych w jedną tabelę\n",
    "* `experiment_for_dataset` - metoda odpowiedzialna za wykonanie eksperymentu dla przekazanych danych. Dla każdego rozmiaru testowego dzieli wykonuje zadaną liczbę iteracji, dzieląc te same dane, a następnie po zadanej iteracji uśrednia zebrane wyniki dla danego rozmiaru zbioru testowego. Wraz z parametrem `verbose` pozwala śledzić na ekranie postęp obliczeń\n",
    "* `clf_experiment` - metoda, która dla przekazanego klasyfikatora oraz danych wykonuje uczenie mierząc jego czas, oraz wyznacza miary `accuracy`, `f1` oraz `auc` zarówno dla zbioru uczącego jak i testowego. Zbiera wszystkie wyniki w całość i zwraca `pd.Series`\n",
    "* `calc_classification_quality` - metoda wykorzystywana przez powyższą do wyznaczania miar\n",
    "* `get_sonar_data` - zwraca zbiór `sonar_csv` jako atrybuty oraz klasy\n",
    "* `get_reuters_data` - zwraca zbiór Reuters: atrybuty jako macierz rzadka oraz klasy jako tablica `numpy`, ponieważ zastosowane klasyfikatory wymagają decyzji jako danych gęstych\n",
    "* `get_mnist_data` - zwraca zbiór mnist: atrybuty jako macierz rzadka oraz wybrana cyfra jako klasa pozytywna, a reszta jako negatywna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e13cddc-f7cd-45df-a472-1be605a1315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifierTest:\n",
    "    def __init__(self, test_sizes: np.array = np.logspace(-1, -0.5, num=2), regularization_params: list = None, results_count_per_test_size: int = 3):\n",
    "        self.max_iter = 1000\n",
    "        self.test_sizes = test_sizes\n",
    "        self.regularization_params = regularization_params if not None else [10 ** i for i in range(-3, 2)]\n",
    "        self.results_count_per_test_size = results_count_per_test_size\n",
    "\n",
    "        penalties = [\n",
    "            'l1',\n",
    "            'l2',\n",
    "            # 'elasticnet'  # only solver saga can handle with it, but it throws error on calc\n",
    "        ]\n",
    "        self.classifiers = {\n",
    "            'svc': SVC(kernel='linear', max_iter=self.max_iter),\n",
    "            # 'svm2': Svm2(),\n",
    "            'mlp': MLPClassifier(hidden_layer_sizes=(), max_iter=self.max_iter),\n",
    "            **{f'logistic_regression_{penalty}': LogisticRegression(penalty=penalty, max_iter=self.max_iter, solver='saga') for penalty in penalties}\n",
    "        }\n",
    "\n",
    "        # uncomment to run with GridSearch. Warning!! Calculating time will drastically increase, especially for sparse data.\n",
    "        # self.classifiers = {\n",
    "        #     'svc': GridSearchCV(SVC(kernel='linear', max_iter=self.max_iter), param_grid={'C': self.regularization_params}),\n",
    "        #     # 'svm2': GridSearchCV(Svm2(), param_grid={'c': self.regularization_params}),\n",
    "        #     'mlp': GridSearchCV(MLPClassifier(hidden_layer_sizes=(), max_iter=self.max_iter), param_grid={'alpha': self.regularization_params}),\n",
    "        #     **{f'logistic_regression_{penalty}': GridSearchCV(LogisticRegression(penalty=penalty, max_iter=self.max_iter, solver='saga'), param_grid={'C': self.regularization_params}) for penalty in penalties}\n",
    "        # }\n",
    "\n",
    "        self.data_table = None\n",
    "\n",
    "    def experiment(self, verbose: bool = False):\n",
    "        dataset_method_generators = {\n",
    "            'sonar': self.get_sonar_data,\n",
    "            'reuters': self.get_reuters_data,\n",
    "            'mnist': self.get_mnist_data,\n",
    "        }\n",
    "\n",
    "        for dataset_name, method in dataset_method_generators.items():\n",
    "            if verbose:\n",
    "                print(f'\\n--------------- {dataset_name} ---------------')\n",
    "\n",
    "            x, y = method()\n",
    "\n",
    "            results = self.experiment_for_dataset(x, y, verbose)\n",
    "            results['dataset'] = dataset_name\n",
    "\n",
    "            self.data_table = results if self.data_table is None else pd.concat([self.data_table, results], ignore_index=True)\n",
    "\n",
    "    def experiment_for_dataset(self, x: np.array, y: np.array, verbose: bool = False, verbose_level: int = 2) -> pd.DataFrame:\n",
    "        mean_results_count = len(self.classifiers.keys()) * self.test_sizes.size\n",
    "        mean_results_storage = [0 for _ in range(mean_results_count)]\n",
    "        mean_results_iter = 0\n",
    "\n",
    "        for test_size_iter, test_size in enumerate(self.test_sizes):\n",
    "            results_storage = {clf_name: [0] * self.results_count_per_test_size for clf_name in self.classifiers.keys()}\n",
    "            t1_test_size = time.time()\n",
    "\n",
    "            for results_iter in range(self.results_count_per_test_size):\n",
    "                separated_data = train_test_split(x, y, test_size=test_size)\n",
    "                t1_inner_loop = time.time()\n",
    "\n",
    "                for clf_iter, (clf_name, clf) in enumerate(self.classifiers.items()):\n",
    "                    t1_clf_loop = time.time()\n",
    "                    experiment_results = self.clf_experiment(clf, separated_data)\n",
    "                    experiment_results['clf'] = clf_name\n",
    "\n",
    "                    results_storage[clf_name][results_iter] = experiment_results\n",
    "                    \n",
    "                    if verbose and verbose_level > 2:\n",
    "                        t2_clf_loop = time.time()\n",
    "                        time_clf_loop = t2_clf_loop - t1_clf_loop\n",
    "                        progres_percent = (clf_iter + 1) / len(self.classifiers.keys()) * 100\n",
    "\n",
    "                if verbose and verbose_level > 1:\n",
    "                    t2_inner_loop = time.time()\n",
    "                    time_inner_loop = t2_inner_loop - t1_inner_loop\n",
    "                    progress_percent = (results_iter + 1) / self.results_count_per_test_size * 100\n",
    "\n",
    "                    print(f'\\t\\tProgress inner loop: {progress_percent:.2f}% - {time_inner_loop:.4f}s')\n",
    "\n",
    "            for results_iter, (clf_name, results_for_clf) in enumerate(results_storage.items()):\n",
    "                results = pd.DataFrame(results_for_clf)\n",
    "                mean_results = results.mean()\n",
    "\n",
    "                mean_results['clf'] = clf_name\n",
    "                mean_results['test_size'] = test_size\n",
    "                mean_results_storage[mean_results_iter] = mean_results\n",
    "\n",
    "                mean_results_iter += 1\n",
    "\n",
    "            if verbose:\n",
    "                t2_test_size = time.time()\n",
    "                time_test_size = t2_test_size - t1_test_size\n",
    "                progress_percent = (test_size_iter + 1) / self.test_sizes.size * 100\n",
    "\n",
    "                print(f'\\tProgress test size: {progress_percent:.2f}% - {time_test_size:.4f}s')\n",
    "\n",
    "        return pd.DataFrame(mean_results_storage)\n",
    "\n",
    "    def clf_experiment(self, clf, separated_data: tuple) -> pd.Series:\n",
    "        x_train, x_test, y_train, y_test = separated_data\n",
    "\n",
    "        t1_fit = time.time()\n",
    "        clf.fit(x_train, y_train)\n",
    "        t2_fit = time.time()\n",
    "        time_fit = t2_fit - t1_fit\n",
    "\n",
    "        classification_quality_train = self.calc_classification_quality(clf, x_train, y_train, 'train')\n",
    "        classification_quality_test = self.calc_classification_quality(clf, x_test, y_test, 'test')\n",
    "\n",
    "        return pd.Series({\n",
    "            'fit_time': time_fit,\n",
    "            **classification_quality_train,\n",
    "            **classification_quality_test\n",
    "        })\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_classification_quality(clf, x: np.array, y: np.array, data_type_label: str = 'train'):\n",
    "        t1_predict = time.time()\n",
    "        y_predicted = clf.predict(x)\n",
    "        t2_predict = time.time()\n",
    "        time_predict = t2_predict - t1_predict\n",
    "\n",
    "        accuracy = metrics.accuracy_score(y, y_predicted)\n",
    "        f1_score = metrics.f1_score(y, y_predicted)\n",
    "        auc_score = metrics.roc_auc_score(y, y_predicted)\n",
    "\n",
    "        return {\n",
    "            f'predict_time_{data_type_label}': time_predict,\n",
    "            f'accuracy_{data_type_label}': accuracy,\n",
    "            f'f1_{data_type_label}': f1_score,\n",
    "            f'auc_{data_type_label}': auc_score,\n",
    "        }\n",
    "\n",
    "    def get_sonar_data(self) -> Tuple:\n",
    "        sonar_data = pd.read_csv('sonar_csv.csv')\n",
    "        y = self.normalize_decisions(sonar_data[sonar_data.columns[-1]])\n",
    "        x = sonar_data.drop(sonar_data.columns[-1], axis=1).to_numpy()\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def get_reuters_data() -> Tuple:\n",
    "        rcv1 = fetch_rcv1()\n",
    "        x = rcv1['data'] > 0\n",
    "        xr = x[:, 2]\n",
    "        y = rcv1['target'][:, 5]\n",
    "\n",
    "        # return xr, y.toarray().ravel()\n",
    "        return x, y.toarray().ravel()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mnist_data(class_digit: int = 5) -> Tuple:\n",
    "        x, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False, parser=\"pandas\")\n",
    "\n",
    "        # return csc_matrix(x)[:, 400], y == str(class_digit)  # 407 column for minimalize sparse\n",
    "        return csc_matrix(x), y == str(class_digit)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_decisions(d) -> np.array:\n",
    "        d_normalized = np.ones(d.size).astype(\"int8\")\n",
    "        d_normalized[d == np.unique(d)[0]] = -1\n",
    "\n",
    "        return d_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83011bc-d528-470f-8218-74516fd8f20e",
   "metadata": {},
   "source": [
    "Funkcje pomocnicze do wykonania oraz przedstawienia wyników eksperymentu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12fd6a1-8c92-47c1-a1c5-bdc9cd2b34c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_linear_classifiers_experiment_results(results: pd.DataFrame):\n",
    "    print('\\nWhole table from experiment:')\n",
    "    print(results.to_markdown())\n",
    "\n",
    "    columns = ['fit_time', 'predict_time_train', 'accuracy_train', 'f1_train', 'auc_train', 'predict_time_test', 'accuracy_test', 'f1_test', 'auc_test', 'test_size']\n",
    "    print('\\nTable with aggregation by: mean, min, max')\n",
    "    print(results.agg({column: ['mean', 'min', 'max'] for column in columns}).to_markdown())\n",
    "\n",
    "    pivot_table = pd.pivot_table(results, values=['accuracy_test', 'f1_test', 'auc_test'], index=['clf', 'test_size'], aggfunc=np.mean)\n",
    "    print('\\nPivot table by classifier and test size')\n",
    "    print(pivot_table)\n",
    "\n",
    "\n",
    "def linear_classifiers_experiment():\n",
    "    linear_classifier_test = LinearClassifierTest()\n",
    "\n",
    "    print('\\n--------------- sonar ---------------')\n",
    "    linear_classifier_test.test_sizes = np.logspace(-1, -0.5, num=30)\n",
    "    linear_classifier_test.results_count_per_test_size = 100\n",
    "\n",
    "    results = linear_classifier_test.experiment_for_dataset(*linear_classifier_test.get_sonar_data(), verbose=True, verbose_level=1)\n",
    "    display_linear_classifiers_experiment_results(results)\n",
    "\n",
    "    print('\\n\\n--------------- mnist ---------------')\n",
    "    linear_classifier_test.test_sizes = np.logspace(-1, -0.5, num=5)\n",
    "    linear_classifier_test.results_count_per_test_size = 5\n",
    "\n",
    "    results = linear_classifier_test.experiment_for_dataset(*LinearClassifierTest.get_mnist_data(), verbose=True)\n",
    "    display_linear_classifiers_experiment_results(results)\n",
    "\n",
    "    print('\\n\\n--------------- reuters ---------------')\n",
    "    linear_classifier_test.test_sizes = np.array([np.logspace(-1, -0.5, num=50)[-20]])\n",
    "    linear_classifier_test.results_count_per_test_size = 1\n",
    "\n",
    "    results = linear_classifier_test.experiment_for_dataset(*LinearClassifierTest.get_reuters_data(), verbose=True, verbose_level=3)\n",
    "    display_linear_classifiers_experiment_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e9f23-e851-40ab-ad52-d306dcb794f4",
   "metadata": {},
   "source": [
    "Wykonanie eksperymentu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd58a7c1-c85d-488e-9850-7a504342d87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- sonar ---------------\n",
      "\tProgress test size: 3.33% - 38.0142s\n",
      "\tProgress test size: 6.67% - 38.4673s\n",
      "\tProgress test size: 10.00% - 37.7358s\n",
      "\tProgress test size: 13.33% - 37.7561s\n",
      "\tProgress test size: 16.67% - 38.0275s\n",
      "\tProgress test size: 20.00% - 37.6024s\n",
      "\tProgress test size: 23.33% - 37.5727s\n",
      "\tProgress test size: 26.67% - 37.3923s\n",
      "\tProgress test size: 30.00% - 37.2516s\n",
      "\tProgress test size: 33.33% - 37.4699s\n",
      "\tProgress test size: 36.67% - 37.3143s\n",
      "\tProgress test size: 40.00% - 37.3763s\n",
      "\tProgress test size: 43.33% - 37.3005s\n",
      "\tProgress test size: 46.67% - 37.4057s\n",
      "\tProgress test size: 50.00% - 37.2849s\n",
      "\tProgress test size: 53.33% - 37.1891s\n",
      "\tProgress test size: 56.67% - 37.2674s\n",
      "\tProgress test size: 60.00% - 37.0793s\n",
      "\tProgress test size: 63.33% - 37.2515s\n",
      "\tProgress test size: 66.67% - 37.2044s\n",
      "\tProgress test size: 70.00% - 37.1108s\n",
      "\tProgress test size: 73.33% - 37.3167s\n",
      "\tProgress test size: 76.67% - 36.9360s\n",
      "\tProgress test size: 80.00% - 36.7224s\n",
      "\tProgress test size: 83.33% - 37.0643s\n",
      "\tProgress test size: 86.67% - 15.7942s\n",
      "\tProgress test size: 90.00% - 15.3843s\n",
      "\tProgress test size: 93.33% - 15.7171s\n",
      "\tProgress test size: 96.67% - 15.4307s\n",
      "\tProgress test size: 100.00% - 15.4276s\n",
      "\n",
      "Whole table from experiment:\n",
      "|     |    fit_time |   predict_time_train |   accuracy_train |   f1_train |   auc_train |   predict_time_test |   accuracy_test |   f1_test |   auc_test | clf                    |   test_size |\n",
      "|----:|------------:|---------------------:|-----------------:|-----------:|------------:|--------------------:|----------------:|----------:|-----------:|:-----------------------|------------:|\n",
      "|   0 | 0.00114111  |          0.000458109 |         0.834706 |   0.810654 |    0.829734 |         0.000119932 |        0.788095 |  0.748972 |   0.786898 | svc                    |    0.1      |\n",
      "|   1 | 0.321578    |          0.000240076 |         0.79385  |   0.760265 |    0.787734 |         5.03254e-05 |        0.74381  |  0.688267 |   0.743313 | mlp                    |    0.1      |\n",
      "|   2 | 0.0378621   |          0.000707228 |         0.828503 |   0.811189 |    0.825838 |         0.0001018   |        0.767143 |  0.73514  |   0.767322 | logistic_regression_l1 |    0.1      |\n",
      "|   3 | 0.0072124   |          0.000701768 |         0.833316 |   0.813971 |    0.829859 |         4.12703e-05 |        0.781429 |  0.748528 |   0.782341 | logistic_regression_l2 |    0.1      |\n",
      "|   4 | 0.0011202   |          0.000548964 |         0.838871 |   0.817513 |    0.834695 |         0.000190535 |        0.785455 |  0.73541  |   0.780157 | svc                    |    0.10405  |\n",
      "|   5 | 0.323994    |          0.000160005 |         0.796559 |   0.765633 |    0.791082 |         0.000110583 |        0.749091 |  0.698664 |   0.748288 | mlp                    |    0.10405  |\n",
      "|   6 | 0.0398725   |          0.000692868 |         0.827366 |   0.811409 |    0.825071 |         0.00010067  |        0.771364 |  0.734516 |   0.769526 | logistic_regression_l1 |    0.10405  |\n",
      "|   7 | 0.00735551  |          0.000707588 |         0.835376 |   0.81804  |    0.832485 |         7.19666e-05 |        0.779091 |  0.73872  |   0.777321 | logistic_regression_l2 |    0.10405  |\n",
      "|   8 | 0.00111977  |          0.000479224 |         0.836378 |   0.813718 |    0.831664 |         0.000110869 |        0.776957 |  0.738959 |   0.774485 | svc                    |    0.108264 |\n",
      "|   9 | 0.320564    |          0.000290005 |         0.795189 |   0.763047 |    0.789206 |         0.000100713 |        0.752174 |  0.706731 |   0.749008 | mlp                    |    0.108264 |\n",
      "|  10 | 0.0363859   |          0.000688612 |         0.828703 |   0.811595 |    0.826026 |         7.12323e-05 |        0.773478 |  0.749861 |   0.776895 | logistic_regression_l1 |    0.108264 |\n",
      "|  11 | 0.00714181  |          0.000656366 |         0.833514 |   0.814774 |    0.830151 |         3.05128e-05 |        0.779565 |  0.752389 |   0.779853 | logistic_regression_l2 |    0.108264 |\n",
      "|  12 | 0.0011149   |          0.000496035 |         0.837935 |   0.81603  |    0.833361 |         0.000149586 |        0.79     |  0.744062 |   0.784912 | svc                    |    0.112648 |\n",
      "|  13 | 0.3197      |          0.00028002  |         0.797065 |   0.766583 |    0.791528 |         6.03127e-05 |        0.759167 |  0.709733 |   0.758084 | mlp                    |    0.112648 |\n",
      "|  14 | 0.0371528   |          0.000750961 |         0.827446 |   0.811509 |    0.825034 |         6.05536e-05 |        0.779583 |  0.749917 |   0.782045 | logistic_regression_l1 |    0.112648 |\n",
      "|  15 | 0.00740152  |          0.000759861 |         0.834402 |   0.816373 |    0.831211 |         5.93829e-05 |        0.787083 |  0.74981  |   0.786292 | logistic_regression_l2 |    0.112648 |\n",
      "|  16 | 0.00153342  |          0.000229855 |         0.837432 |   0.815698 |    0.833106 |         0.000176246 |        0.778    |  0.741063 |   0.774007 | svc                    |    0.11721  |\n",
      "|  17 | 0.323413    |          6.00195e-05 |         0.795301 |   0.7626   |    0.789221 |         0.000236707 |        0.756    |  0.712683 |   0.753807 | mlp                    |    0.11721  |\n",
      "|  18 | 0.0371659   |          0.000590167 |         0.83082  |   0.814386 |    0.828359 |         2.99883e-05 |        0.774    |  0.750209 |   0.773136 | logistic_regression_l1 |    0.11721  |\n",
      "|  19 | 0.00778393  |          0.000528867 |         0.834918 |   0.816183 |    0.831586 |         9.98497e-06 |        0.778    |  0.748209 |   0.776096 | logistic_regression_l2 |    0.11721  |\n",
      "|  20 | 0.000957713 |          0.000478895 |         0.837473 |   0.815843 |    0.833154 |         0           |        0.783077 |  0.747035 |   0.781322 | svc                    |    0.121957 |\n",
      "|  21 | 0.320494    |          0.000156248 |         0.796648 |   0.765157 |    0.790954 |         0           |        0.746923 |  0.702072 |   0.744684 | mlp                    |    0.121957 |\n",
      "|  22 | 0.0360919   |          0.000625012 |         0.827692 |   0.811161 |    0.825316 |         0.000156584 |        0.775    |  0.747784 |   0.774829 | logistic_regression_l1 |    0.121957 |\n",
      "|  23 | 0.00749212  |          0.000321774 |         0.832857 |   0.813585 |    0.829487 |         0           |        0.787692 |  0.757688 |   0.786481 | logistic_regression_l2 |    0.121957 |\n",
      "|  24 | 0.00111382  |          0.000468743 |         0.839171 |   0.81684  |    0.834477 |         1.00303e-05 |        0.780741 |  0.74239  |   0.780132 | svc                    |    0.126896 |\n",
      "|  25 | 0.321034    |          0           |         0.79558  |   0.763958 |    0.789861 |         0           |        0.742222 |  0.693952 |   0.742519 | mlp                    |    0.126896 |\n",
      "|  26 | 0.0359492   |          0.000196006 |         0.829227 |   0.812582 |    0.826724 |         0           |        0.772222 |  0.743333 |   0.774828 | logistic_regression_l1 |    0.126896 |\n",
      "|  27 | 0.00637952  |          0.000669127 |         0.836796 |   0.81826  |    0.833448 |         2.06184e-05 |        0.77037  |  0.738687 |   0.772974 | logistic_regression_l2 |    0.126896 |\n",
      "|  28 | 0.00126976  |          0.000322754 |         0.839722 |   0.818055 |    0.835267 |         0           |        0.787143 |  0.740245 |   0.781213 | svc                    |    0.132035 |\n",
      "|  29 | 0.319302    |          0.00015625  |         0.7975   |   0.766968 |    0.791943 |         0           |        0.753571 |  0.702399 |   0.749127 | mlp                    |    0.132035 |\n",
      "|  30 | 0.0359761   |          0.000977004 |         0.826389 |   0.810549 |    0.824093 |         0.000156476 |        0.782857 |  0.749904 |   0.783791 | logistic_regression_l1 |    0.132035 |\n",
      "|  31 | 0.00679616  |          0.000636935 |         0.834333 |   0.81676  |    0.831265 |         0           |        0.785    |  0.74834  |   0.782731 | logistic_regression_l2 |    0.132035 |\n",
      "|  32 | 0.000811839 |          0.000488415 |         0.837039 |   0.815586 |    0.832768 |         0.000156243 |        0.774828 |  0.732409 |   0.771382 | svc                    |    0.137382 |\n",
      "|  33 | 0.318261    |          0.000478737 |         0.795978 |   0.764318 |    0.79008  |         0.000156319 |        0.744483 |  0.695641 |   0.742723 | mlp                    |    0.137382 |\n",
      "|  34 | 0.0353362   |          0.00063458  |         0.829274 |   0.812944 |    0.82672  |         0.000166256 |        0.764138 |  0.732117 |   0.762818 | logistic_regression_l1 |    0.137382 |\n",
      "|  35 | 0.0067742   |          0.000956123 |         0.833855 |   0.815475 |    0.830542 |         0           |        0.778276 |  0.746718 |   0.777976 | logistic_regression_l2 |    0.137382 |\n",
      "|  36 | 0.00143574  |          0.000488925 |         0.838989 |   0.815723 |    0.833916 |         1.00017e-05 |        0.777333 |  0.743057 |   0.778611 | svc                    |    0.142946 |\n",
      "|  37 | 0.320432    |          0.000312502 |         0.79736  |   0.764438 |    0.791077 |         0           |        0.732333 |  0.682685 |   0.734732 | mlp                    |    0.142946 |\n",
      "|  38 | 0.034329    |          0.000967677 |         0.82882  |   0.811031 |    0.82582  |         0           |        0.767667 |  0.743071 |   0.771876 | logistic_regression_l1 |    0.142946 |\n",
      "|  39 | 0.00724161  |          0.000801761 |         0.834719 |   0.815807 |    0.831259 |         1.00327e-05 |        0.771667 |  0.744149 |   0.776505 | logistic_regression_l2 |    0.142946 |\n",
      "|  40 | 0.000166202 |          0.000781536 |         0.841243 |   0.819077 |    0.836454 |         0.000312533 |        0.779032 |  0.740654 |   0.776582 | svc                    |    0.148735 |\n",
      "|  41 | 0.319033    |          1.0004e-05  |         0.800508 |   0.769101 |    0.794523 |         0.000156319 |        0.736129 |  0.689657 |   0.735962 | mlp                    |    0.148735 |\n",
      "|  42 | 0.0332334   |          0.00111478  |         0.83209  |   0.815398 |    0.829408 |         0.000156491 |        0.761613 |  0.736866 |   0.762542 | logistic_regression_l1 |    0.148735 |\n",
      "|  43 | 0.00718407  |          0.000170772 |         0.837232 |   0.81866  |    0.833751 |         1.03593e-05 |        0.770968 |  0.741995 |   0.772085 | logistic_regression_l2 |    0.148735 |\n",
      "|  44 | 0.000473406 |          0.000479338 |         0.838514 |   0.815653 |    0.83364  |         0.000312152 |        0.773636 |  0.735459 |   0.773512 | svc                    |    0.154759 |\n",
      "|  45 | 0.319026    |          0.000468743 |         0.797314 |   0.76493  |    0.791041 |         0.000166433 |        0.73697  |  0.688412 |   0.737475 | mlp                    |    0.154759 |\n",
      "|  46 | 0.0345001   |          0.000781579 |         0.830229 |   0.813126 |    0.827486 |         0           |        0.76697  |  0.741471 |   0.768526 | logistic_regression_l1 |    0.154759 |\n",
      "|  47 | 0.0066095   |          0.000937202 |         0.835143 |   0.816379 |    0.831679 |         0           |        0.764242 |  0.735234 |   0.767047 | logistic_regression_l2 |    0.154759 |\n",
      "|  48 | 0.000984037 |          0.000946653 |         0.837644 |   0.814921 |    0.832823 |         0.000166461 |        0.771176 |  0.731521 |   0.769559 | svc                    |    0.161026 |\n",
      "|  49 | 0.317877    |          0.000312498 |         0.802586 |   0.772156 |    0.79677  |         0           |        0.741471 |  0.692772 |   0.73951  | mlp                    |    0.161026 |\n",
      "|  50 | 0.0354213   |          0.000480082 |         0.826207 |   0.808802 |    0.823354 |         0           |        0.773529 |  0.744349 |   0.774366 | logistic_regression_l1 |    0.161026 |\n",
      "|  51 | 0.00566561  |          0.000651333 |         0.835    |   0.816388 |    0.831585 |         0.000156245 |        0.771176 |  0.73895  |   0.770921 | logistic_regression_l2 |    0.161026 |\n",
      "|  52 | 0.00048857  |          0.000312369 |         0.837688 |   0.816096 |    0.83328  |         0.000469036 |        0.784571 |  0.746607 |   0.782724 | svc                    |    0.167547 |\n",
      "|  53 | 0.320322    |          0.000320747 |         0.794393 |   0.762371 |    0.788442 |         0           |        0.754857 |  0.707065 |   0.75125  | mlp                    |    0.167547 |\n",
      "|  54 | 0.0338246   |          0.00109335  |         0.827052 |   0.810611 |    0.824431 |         0           |        0.770286 |  0.743991 |   0.771636 | logistic_regression_l1 |    0.167547 |\n",
      "|  55 | 0.00678456  |          0.000634634 |         0.834335 |   0.81628  |    0.831068 |         0           |        0.778571 |  0.747665 |   0.778562 | logistic_regression_l2 |    0.167547 |\n",
      "|  56 | 0.000488732 |          0.00046874  |         0.838421 |   0.815777 |    0.833514 |         0.000312879 |        0.774324 |  0.734953 |   0.771094 | svc                    |    0.174333 |\n",
      "|  57 | 0.319462    |          0.000157912 |         0.801696 |   0.770311 |    0.79546  |         0.000312645 |        0.741081 |  0.69316  |   0.739665 | mlp                    |    0.174333 |\n",
      "|  58 | 0.0342776   |          0.000479524 |         0.828012 |   0.810997 |    0.825043 |         0           |        0.765946 |  0.737513 |   0.765838 | logistic_regression_l1 |    0.174333 |\n",
      "|  59 | 0.00710745  |          0.00110446  |         0.836316 |   0.817431 |    0.832533 |         0.000156229 |        0.766486 |  0.736931 |   0.766888 | logistic_regression_l2 |    0.174333 |\n",
      "|  60 | 0.000635517 |          0.000312612 |         0.840588 |   0.819271 |    0.836136 |         0           |        0.77     |  0.730513 |   0.767452 | svc                    |    0.181393 |\n",
      "|  61 | 0.318835    |          0.000478749 |         0.795235 |   0.762645 |    0.789197 |         0.000156336 |        0.746842 |  0.701893 |   0.744184 | mlp                    |    0.181393 |\n",
      "|  62 | 0.033477    |          0.000947402 |         0.830647 |   0.814782 |    0.82823  |         0.000166874 |        0.763947 |  0.737217 |   0.764729 | logistic_regression_l1 |    0.181393 |\n",
      "|  63 | 0.00637044  |          0.000625293 |         0.836941 |   0.818931 |    0.83366  |         0           |        0.766579 |  0.738355 |   0.766649 | logistic_regression_l2 |    0.181393 |\n",
      "|  64 | 0.000957704 |          0.000332484 |         0.840238 |   0.820028 |    0.836084 |         1.00207e-05 |        0.77625  |  0.732467 |   0.76995  | svc                    |    0.188739 |\n",
      "|  65 | 0.320303    |          0           |         0.80006  |   0.770413 |    0.794469 |         0           |        0.7455   |  0.695218 |   0.740938 | mlp                    |    0.188739 |\n",
      "|  66 | 0.0336742   |          0.000166395 |         0.825595 |   0.810042 |    0.823115 |         0           |        0.76725  |  0.737526 |   0.766627 | logistic_regression_l1 |    0.188739 |\n",
      "|  67 | 0.00673198  |          0.000655646 |         0.835595 |   0.818174 |    0.832347 |         0           |        0.7725   |  0.738907 |   0.770077 | logistic_regression_l2 |    0.188739 |\n",
      "|  68 | 0.000791426 |          0.000478652 |         0.838743 |   0.816042 |    0.833839 |         0.000166254 |        0.77     |  0.735194 |   0.768314 | svc                    |    0.196383 |\n",
      "|  69 | 0.32054     |          0           |         0.800898 |   0.769906 |    0.794851 |         0.00031265  |        0.740244 |  0.696947 |   0.740152 | mlp                    |    0.196383 |\n",
      "|  70 | 0.0326789   |          0.000166032 |         0.828623 |   0.812046 |    0.825883 |         0           |        0.75878  |  0.734787 |   0.759892 | logistic_regression_l1 |    0.196383 |\n",
      "|  71 | 0.00793072  |          0.000316703 |         0.835928 |   0.817028 |    0.832286 |         0           |        0.769024 |  0.741525 |   0.769566 | logistic_regression_l2 |    0.196383 |\n",
      "|  72 | 0.00067538  |          0.000498769 |         0.840303 |   0.81924  |    0.835767 |         0.000156221 |        0.772791 |  0.7351   |   0.77112  | svc                    |    0.204336 |\n",
      "|  73 | 0.318976    |          0.000156252 |         0.801515 |   0.771494 |    0.795662 |         0           |        0.743953 |  0.700919 |   0.741761 | mlp                    |    0.204336 |\n",
      "|  74 | 0.0347061   |          0.000501945 |         0.831212 |   0.815597 |    0.828633 |         0.000161057 |        0.763953 |  0.736322 |   0.765318 | logistic_regression_l1 |    0.204336 |\n",
      "|  75 | 0.0056579   |          0.000972338 |         0.838364 |   0.820787 |    0.83494  |         0.000322754 |        0.768372 |  0.740226 |   0.769916 | logistic_regression_l2 |    0.204336 |\n",
      "|  76 | 0.000342996 |          5.25713e-06 |         0.84411  |   0.823449 |    0.839577 |         0.000478697 |        0.766222 |  0.722009 |   0.762424 | svc                    |    0.212611 |\n",
      "|  77 | 0.319421    |          0.000166249 |         0.804479 |   0.774927 |    0.798621 |         0           |        0.732    |  0.67824  |   0.730353 | mlp                    |    0.212611 |\n",
      "|  78 | 0.0331576   |          0.000781744 |         0.831104 |   0.814946 |    0.828254 |         0           |        0.756444 |  0.725085 |   0.756383 | logistic_regression_l1 |    0.212611 |\n",
      "|  79 | 0.00667928  |          0.000182002 |         0.838834 |   0.821045 |    0.835325 |         0           |        0.761556 |  0.72595  |   0.760633 | logistic_regression_l2 |    0.212611 |\n",
      "|  80 | 0.00142657  |          0.000478458 |         0.843168 |   0.821476 |    0.83834  |         0.000156245 |        0.759574 |  0.720094 |   0.757596 | svc                    |    0.221222 |\n",
      "|  81 | 0.320022    |          0.00031249  |         0.80354  |   0.773655 |    0.797484 |         0.000156322 |        0.730213 |  0.685408 |   0.729296 | mlp                    |    0.221222 |\n",
      "|  82 | 0.0324461   |          0.000626161 |         0.827205 |   0.810541 |    0.824161 |         0.000156476 |        0.755532 |  0.72958  |   0.756286 | logistic_regression_l1 |    0.221222 |\n",
      "|  83 | 0.005304    |          0.00047869  |         0.836025 |   0.81772  |    0.83226  |         0           |        0.765319 |  0.736428 |   0.766004 | logistic_regression_l2 |    0.221222 |\n",
      "|  84 | 0.000816863 |          0.000322161 |         0.845125 |   0.824178 |    0.840424 |         0.000312812 |        0.761042 |  0.720839 |   0.758711 | svc                    |    0.230181 |\n",
      "|  85 | 0.321114    |          0.000644994 |         0.804125 |   0.774037 |    0.798062 |         0           |        0.738542 |  0.693912 |   0.736842 | mlp                    |    0.230181 |\n",
      "|  86 | 0.0329769   |          0.000489349 |         0.831625 |   0.81502  |    0.828487 |         0.000156476 |        0.760625 |  0.735703 |   0.761077 | logistic_regression_l1 |    0.230181 |\n",
      "|  87 | 0.00643763  |          0.000176566 |         0.83825  |   0.819757 |    0.834516 |         0.000156267 |        0.759167 |  0.72735  |   0.759376 | logistic_regression_l2 |    0.230181 |\n",
      "|  88 | 0.00158657  |          0.00031251  |         0.842595 |   0.820742 |    0.837744 |         0           |        0.772    |  0.735693 |   0.769247 | svc                    |    0.239503 |\n",
      "|  89 | 0.319123    |          0.00015625  |         0.800253 |   0.767472 |    0.793701 |         0           |        0.7388   |  0.696803 |   0.737202 | mlp                    |    0.239503 |\n",
      "|  90 | 0.0314229   |          0.00172907  |         0.826962 |   0.809385 |    0.823868 |         0           |        0.767    |  0.743481 |   0.767576 | logistic_regression_l1 |    0.239503 |\n",
      "|  91 | 0.00614803  |          0.000479231 |         0.838165 |   0.819826 |    0.834581 |         0           |        0.7678   |  0.739384 |   0.767771 | logistic_regression_l2 |    0.239503 |\n",
      "|  92 | 0.00110343  |          0.000468435 |         0.845513 |   0.824553 |    0.840743 |         0.000322618 |        0.767692 |  0.730663 |   0.765563 | svc                    |    0.249202 |\n",
      "|  93 | 0.316591    |          0.000156257 |         0.804359 |   0.77406  |    0.797936 |         0.000156326 |        0.736154 |  0.688977 |   0.734317 | mlp                    |    0.249202 |\n",
      "|  94 | 0.031429    |          0.000937772 |         0.831859 |   0.814783 |    0.828513 |         0.00015626  |        0.760385 |  0.735191 |   0.760923 | logistic_regression_l1 |    0.249202 |\n",
      "|  95 | 0.00563166  |          0.000479131 |         0.841346 |   0.823272 |    0.837413 |         3.32594e-06 |        0.768654 |  0.73905  |   0.768671 | logistic_regression_l2 |    0.249202 |\n",
      "|  96 | 0.000811687 |          0.00048893  |         0.844286 |   0.823074 |    0.839525 |         0.00017597  |        0.775926 |  0.73779  |   0.773047 | svc                    |    0.259294 |\n",
      "|  97 | 0.31936     |          0.000478759 |         0.804351 |   0.775861 |    0.798555 |         0           |        0.743148 |  0.70012  |   0.741307 | mlp                    |    0.259294 |\n",
      "|  98 | 0.0325274   |          0.000635459 |         0.825649 |   0.809792 |    0.822811 |         0           |        0.767963 |  0.74169  |   0.769331 | logistic_regression_l1 |    0.259294 |\n",
      "|  99 | 0.00607894  |          0.000812984 |         0.836948 |   0.818858 |    0.833207 |         0           |        0.76963  |  0.739691 |   0.769869 | logistic_regression_l2 |    0.259294 |\n",
      "| 100 | 0.00145046  |          0           |         0.84298  |   0.822857 |    0.838584 |         0.00031256  |        0.760351 |  0.726546 |   0.758625 | svc                    |    0.269795 |\n",
      "| 101 | 0.109753    |          0.000322475 |         0.806225 |   0.777595 |    0.800319 |         0           |        0.735789 |  0.696563 |   0.734751 | mlp                    |    0.269795 |\n",
      "| 102 | 0.0310967   |          0.000312781 |         0.830464 |   0.814618 |    0.827592 |         0           |        0.756667 |  0.730296 |   0.757438 | logistic_regression_l1 |    0.269795 |\n",
      "| 103 | 0.00607462  |          9.99928e-06 |         0.838477 |   0.820693 |    0.834691 |         0           |        0.758947 |  0.731451 |   0.759586 | logistic_regression_l2 |    0.269795 |\n",
      "| 104 | 0.000655086 |          0.000312512 |         0.844497 |   0.824234 |    0.839984 |         0.000166545 |        0.768475 |  0.73283  |   0.766214 | svc                    |    0.280722 |\n",
      "| 105 | 0.107715    |          0           |         0.805772 |   0.77643  |    0.799773 |         0           |        0.738983 |  0.695147 |   0.737398 | mlp                    |    0.280722 |\n",
      "| 106 | 0.0295319   |          0.000156276 |         0.82604  |   0.808643 |    0.822728 |         0.000156248 |        0.758305 |  0.733363 |   0.759595 | logistic_regression_l1 |    0.280722 |\n",
      "| 107 | 0.00585718  |          0           |         0.83651  |   0.817844 |    0.832703 |         0           |        0.769831 |  0.742241 |   0.769658 | logistic_regression_l2 |    0.280722 |\n",
      "| 108 | 0.000648713 |          0.000176291 |         0.849592 |   0.830952 |    0.845365 |         1.00064e-05 |        0.756885 |  0.716229 |   0.752476 | svc                    |    0.29209  |\n",
      "| 109 | 0.110075    |          0           |         0.807211 |   0.7792   |    0.801293 |         0.00015635  |        0.732459 |  0.683885 |   0.729009 | mlp                    |    0.29209  |\n",
      "| 110 | 0.0309626   |          0           |         0.832585 |   0.816755 |    0.829416 |         0           |        0.74918  |  0.721325 |   0.748812 | logistic_regression_l1 |    0.29209  |\n",
      "| 111 | 0.00660684  |          0.000156243 |         0.843469 |   0.826541 |    0.839791 |         0.00015624  |        0.755082 |  0.72428  |   0.754309 | logistic_regression_l2 |    0.29209  |\n",
      "| 112 | 0.000791361 |          0.000469029 |         0.849306 |   0.826497 |    0.843614 |         0.00015626  |        0.761094 |  0.727051 |   0.759298 | svc                    |    0.30392  |\n",
      "| 113 | 0.108659    |          0           |         0.810694 |   0.778305 |    0.803224 |         0           |        0.737031 |  0.698378 |   0.737785 | mlp                    |    0.30392  |\n",
      "| 114 | 0.0283444   |          0.000312166 |         0.829444 |   0.809383 |    0.825209 |         0.000312479 |        0.765156 |  0.745498 |   0.767589 | logistic_regression_l1 |    0.30392  |\n",
      "| 115 | 0.00653948  |          0           |         0.841875 |   0.820881 |    0.836736 |         0           |        0.765156 |  0.740399 |   0.766727 | logistic_regression_l2 |    0.30392  |\n",
      "| 116 | 0.00126838  |          0.000166223 |         0.845986 |   0.827682 |    0.841979 |         0.000624704 |        0.762121 |  0.726117 |   0.759197 | svc                    |    0.316228 |\n",
      "| 117 | 0.10878     |          0           |         0.812887 |   0.787021 |    0.8075   |         0           |        0.737121 |  0.69346  |   0.734411 | mlp                    |    0.316228 |\n",
      "| 118 | 0.0280087   |          0           |         0.829859 |   0.815095 |    0.827165 |         0.000156257 |        0.752273 |  0.727086 |   0.75253  | logistic_regression_l1 |    0.316228 |\n",
      "| 119 | 0.00653635  |          0.000156262 |         0.84162  |   0.825255 |    0.838056 |         0           |        0.756364 |  0.726189 |   0.755952 | logistic_regression_l2 |    0.316228 |\n",
      "\n",
      "Table with aggregation by: mean, min, max\n",
      "|      |    fit_time |   predict_time_train |   accuracy_train |   f1_train |   auc_train |   predict_time_test |   accuracy_test |   f1_test |   auc_test |   test_size |\n",
      "|:-----|------------:|---------------------:|-----------------:|-----------:|------------:|--------------------:|----------------:|----------:|-----------:|------------:|\n",
      "| mean | 0.081544    |           0.00044024 |         0.826788 |   0.804967 |    0.822553 |         9.55094e-05 |        0.763039 |  0.727178 |   0.762238 |    0.188514 |\n",
      "| min  | 0.000166202 |           0          |         0.79385  |   0.760265 |    0.787734 |         0           |        0.730213 |  0.67824  |   0.729009 |    0.1      |\n",
      "| max  | 0.323994    |           0.00172907 |         0.849592 |   0.830952 |    0.845365 |         0.000624704 |        0.79     |  0.757688 |   0.786898 |    0.316228 |\n",
      "\n",
      "Pivot table by classifier and test size\n",
      "                                  accuracy_test  auc_test   f1_test\n",
      "clf                    test_size                                   \n",
      "logistic_regression_l1 0.100000        0.767143  0.767322  0.735140\n",
      "                       0.104050        0.771364  0.769526  0.734516\n",
      "                       0.108264        0.773478  0.776895  0.749861\n",
      "                       0.112648        0.779583  0.782045  0.749917\n",
      "                       0.117210        0.774000  0.773136  0.750209\n",
      "                       0.121957        0.775000  0.774829  0.747784\n",
      "                       0.126896        0.772222  0.774828  0.743333\n",
      "                       0.132035        0.782857  0.783791  0.749904\n",
      "                       0.137382        0.764138  0.762818  0.732117\n",
      "                       0.142946        0.767667  0.771876  0.743071\n",
      "                       0.148735        0.761613  0.762542  0.736866\n",
      "                       0.154759        0.766970  0.768526  0.741471\n",
      "                       0.161026        0.773529  0.774366  0.744349\n",
      "                       0.167547        0.770286  0.771636  0.743991\n",
      "                       0.174333        0.765946  0.765838  0.737513\n",
      "                       0.181393        0.763947  0.764729  0.737217\n",
      "                       0.188739        0.767250  0.766627  0.737526\n",
      "                       0.196383        0.758780  0.759892  0.734787\n",
      "                       0.204336        0.763953  0.765318  0.736322\n",
      "                       0.212611        0.756444  0.756383  0.725085\n",
      "                       0.221222        0.755532  0.756286  0.729580\n",
      "                       0.230181        0.760625  0.761077  0.735703\n",
      "                       0.239503        0.767000  0.767576  0.743481\n",
      "                       0.249202        0.760385  0.760923  0.735191\n",
      "                       0.259294        0.767963  0.769331  0.741690\n",
      "                       0.269795        0.756667  0.757438  0.730296\n",
      "                       0.280722        0.758305  0.759595  0.733363\n",
      "                       0.292090        0.749180  0.748812  0.721325\n",
      "                       0.303920        0.765156  0.767589  0.745498\n",
      "                       0.316228        0.752273  0.752530  0.727086\n",
      "logistic_regression_l2 0.100000        0.781429  0.782341  0.748528\n",
      "                       0.104050        0.779091  0.777321  0.738720\n",
      "                       0.108264        0.779565  0.779853  0.752389\n",
      "                       0.112648        0.787083  0.786292  0.749810\n",
      "                       0.117210        0.778000  0.776096  0.748209\n",
      "                       0.121957        0.787692  0.786481  0.757688\n",
      "                       0.126896        0.770370  0.772974  0.738687\n",
      "                       0.132035        0.785000  0.782731  0.748340\n",
      "                       0.137382        0.778276  0.777976  0.746718\n",
      "                       0.142946        0.771667  0.776505  0.744149\n",
      "                       0.148735        0.770968  0.772085  0.741995\n",
      "                       0.154759        0.764242  0.767047  0.735234\n",
      "                       0.161026        0.771176  0.770921  0.738950\n",
      "                       0.167547        0.778571  0.778562  0.747665\n",
      "                       0.174333        0.766486  0.766888  0.736931\n",
      "                       0.181393        0.766579  0.766649  0.738355\n",
      "                       0.188739        0.772500  0.770077  0.738907\n",
      "                       0.196383        0.769024  0.769566  0.741525\n",
      "                       0.204336        0.768372  0.769916  0.740226\n",
      "                       0.212611        0.761556  0.760633  0.725950\n",
      "                       0.221222        0.765319  0.766004  0.736428\n",
      "                       0.230181        0.759167  0.759376  0.727350\n",
      "                       0.239503        0.767800  0.767771  0.739384\n",
      "                       0.249202        0.768654  0.768671  0.739050\n",
      "                       0.259294        0.769630  0.769869  0.739691\n",
      "                       0.269795        0.758947  0.759586  0.731451\n",
      "                       0.280722        0.769831  0.769658  0.742241\n",
      "                       0.292090        0.755082  0.754309  0.724280\n",
      "                       0.303920        0.765156  0.766727  0.740399\n",
      "                       0.316228        0.756364  0.755952  0.726189\n",
      "mlp                    0.100000        0.743810  0.743313  0.688267\n",
      "                       0.104050        0.749091  0.748288  0.698664\n",
      "                       0.108264        0.752174  0.749008  0.706731\n",
      "                       0.112648        0.759167  0.758084  0.709733\n",
      "                       0.117210        0.756000  0.753807  0.712683\n",
      "                       0.121957        0.746923  0.744684  0.702072\n",
      "                       0.126896        0.742222  0.742519  0.693952\n",
      "                       0.132035        0.753571  0.749127  0.702399\n",
      "                       0.137382        0.744483  0.742723  0.695641\n",
      "                       0.142946        0.732333  0.734732  0.682685\n",
      "                       0.148735        0.736129  0.735962  0.689657\n",
      "                       0.154759        0.736970  0.737475  0.688412\n",
      "                       0.161026        0.741471  0.739510  0.692772\n",
      "                       0.167547        0.754857  0.751250  0.707065\n",
      "                       0.174333        0.741081  0.739665  0.693160\n",
      "                       0.181393        0.746842  0.744184  0.701893\n",
      "                       0.188739        0.745500  0.740938  0.695218\n",
      "                       0.196383        0.740244  0.740152  0.696947\n",
      "                       0.204336        0.743953  0.741761  0.700919\n",
      "                       0.212611        0.732000  0.730353  0.678240\n",
      "                       0.221222        0.730213  0.729296  0.685408\n",
      "                       0.230181        0.738542  0.736842  0.693912\n",
      "                       0.239503        0.738800  0.737202  0.696803\n",
      "                       0.249202        0.736154  0.734317  0.688977\n",
      "                       0.259294        0.743148  0.741307  0.700120\n",
      "                       0.269795        0.735789  0.734751  0.696563\n",
      "                       0.280722        0.738983  0.737398  0.695147\n",
      "                       0.292090        0.732459  0.729009  0.683885\n",
      "                       0.303920        0.737031  0.737785  0.698378\n",
      "                       0.316228        0.737121  0.734411  0.693460\n",
      "svc                    0.100000        0.788095  0.786898  0.748972\n",
      "                       0.104050        0.785455  0.780157  0.735410\n",
      "                       0.108264        0.776957  0.774485  0.738959\n",
      "                       0.112648        0.790000  0.784912  0.744062\n",
      "                       0.117210        0.778000  0.774007  0.741063\n",
      "                       0.121957        0.783077  0.781322  0.747035\n",
      "                       0.126896        0.780741  0.780132  0.742390\n",
      "                       0.132035        0.787143  0.781213  0.740245\n",
      "                       0.137382        0.774828  0.771382  0.732409\n",
      "                       0.142946        0.777333  0.778611  0.743057\n",
      "                       0.148735        0.779032  0.776582  0.740654\n",
      "                       0.154759        0.773636  0.773512  0.735459\n",
      "                       0.161026        0.771176  0.769559  0.731521\n",
      "                       0.167547        0.784571  0.782724  0.746607\n",
      "                       0.174333        0.774324  0.771094  0.734953\n",
      "                       0.181393        0.770000  0.767452  0.730513\n",
      "                       0.188739        0.776250  0.769950  0.732467\n",
      "                       0.196383        0.770000  0.768314  0.735194\n",
      "                       0.204336        0.772791  0.771120  0.735100\n",
      "                       0.212611        0.766222  0.762424  0.722009\n",
      "                       0.221222        0.759574  0.757596  0.720094\n",
      "                       0.230181        0.761042  0.758711  0.720839\n",
      "                       0.239503        0.772000  0.769247  0.735693\n",
      "                       0.249202        0.767692  0.765563  0.730663\n",
      "                       0.259294        0.775926  0.773047  0.737790\n",
      "                       0.269795        0.760351  0.758625  0.726546\n",
      "                       0.280722        0.768475  0.766214  0.732830\n",
      "                       0.292090        0.756885  0.752476  0.716229\n",
      "                       0.303920        0.761094  0.759298  0.727051\n",
      "                       0.316228        0.762121  0.759197  0.726117\n",
      "\n",
      "\n",
      "--------------- mnist ---------------\n",
      "\t\tProgress inner loop: 20.00% - 416.4372s\n",
      "\t\tProgress inner loop: 40.00% - 414.3641s\n",
      "\t\tProgress inner loop: 60.00% - 415.5051s\n",
      "\t\tProgress inner loop: 80.00% - 414.2182s\n",
      "\t\tProgress inner loop: 100.00% - 414.0119s\n",
      "\tProgress test size: 20.00% - 2075.2946s\n",
      "\t\tProgress inner loop: 20.00% - 402.7032s\n",
      "\t\tProgress inner loop: 40.00% - 402.3590s\n",
      "\t\tProgress inner loop: 60.00% - 400.1364s\n",
      "\t\tProgress inner loop: 80.00% - 399.5536s\n",
      "\t\tProgress inner loop: 100.00% - 400.4217s\n",
      "\tProgress test size: 40.00% - 2005.9484s\n",
      "\t\tProgress inner loop: 20.00% - 381.8500s\n",
      "\t\tProgress inner loop: 40.00% - 400.6717s\n",
      "\t\tProgress inner loop: 60.00% - 407.0374s\n",
      "\t\tProgress inner loop: 80.00% - 388.6450s\n",
      "\t\tProgress inner loop: 100.00% - 381.8040s\n",
      "\tProgress test size: 60.00% - 1960.8017s\n",
      "\t\tProgress inner loop: 20.00% - 360.0078s\n",
      "\t\tProgress inner loop: 40.00% - 357.4872s\n",
      "\t\tProgress inner loop: 60.00% - 356.6893s\n",
      "\t\tProgress inner loop: 80.00% - 359.6045s\n",
      "\t\tProgress inner loop: 100.00% - 358.1152s\n",
      "\tProgress test size: 80.00% - 1792.6629s\n",
      "\t\tProgress inner loop: 20.00% - 323.1838s\n",
      "\t\tProgress inner loop: 40.00% - 322.2568s\n",
      "\t\tProgress inner loop: 60.00% - 324.5382s\n",
      "\t\tProgress inner loop: 80.00% - 322.9992s\n",
      "\t\tProgress inner loop: 100.00% - 324.0237s\n",
      "\tProgress test size: 100.00% - 1617.7476s\n",
      "\n",
      "Whole table from experiment:\n",
      "|    |   fit_time |   predict_time_train |   accuracy_train |   f1_train |   auc_train |   predict_time_test |   accuracy_test |   f1_test |   auc_test | clf                    |   test_size |\n",
      "|---:|-----------:|---------------------:|-----------------:|-----------:|------------:|--------------------:|----------------:|----------:|-----------:|:-----------------------|------------:|\n",
      "|  0 |   58.9327  |           51.1095    |         0.73214  |   0.298375 |    0.672816 |          5.69332    |        0.7304   |  0.295967 |   0.673259 | svc                    |    0.1      |\n",
      "|  1 |    2.57902 |            0.0169377 |         0.953229 |   0.744922 |    0.849007 |          0          |        0.951257 |  0.731996 |   0.843745 | mlp                    |    0.1      |\n",
      "|  2 |  230.479   |            0.015626  |         0.978213 |   0.873777 |    0.913924 |          0          |        0.975057 |  0.852201 |   0.8983   | logistic_regression_l1 |    0.1      |\n",
      "|  3 |   65.9857  |            0.0156194 |         0.978225 |   0.873851 |    0.913963 |          0          |        0.975057 |  0.852201 |   0.8983   | logistic_regression_l2 |    0.1      |\n",
      "|  4 |   56.4895  |           49.0087    |         0.720135 |   0.277856 |    0.65162  |          7.53477    |        0.718243 |  0.275323 |   0.651447 | svc                    |    0.133352 |\n",
      "|  5 |    2.47351 |            0.0249968 |         0.956176 |   0.754303 |    0.865281 |          0          |        0.953037 |  0.735525 |   0.857372 | mlp                    |    0.133352 |\n",
      "|  6 |  221.746   |            0.0218745 |         0.977928 |   0.871885 |    0.912355 |          0          |        0.976004 |  0.860075 |   0.906167 | logistic_regression_l1 |    0.133352 |\n",
      "|  7 |   63.6366  |            0.0218749 |         0.977918 |   0.87183  |    0.912333 |          0.00372925 |        0.976004 |  0.860075 |   0.906167 | logistic_regression_l2 |    0.133352 |\n",
      "|  8 |   54.3677  |           47.2257    |         0.705272 |   0.268703 |    0.63566  |         10.0975     |        0.704515 |  0.27251  |   0.639872 | svc                    |    0.177828 |\n",
      "|  9 |    3.43281 |            0.0222534 |         0.964738 |   0.78195  |    0.854382 |          0.00672626 |        0.963094 |  0.77722  |   0.853774 | mlp                    |    0.177828 |\n",
      "| 10 |  215.209   |            0.0181265 |         0.978315 |   0.873976 |    0.913643 |          0.00339894 |        0.975177 |  0.857069 |   0.90565  | logistic_regression_l1 |    0.177828 |\n",
      "| 11 |   61.5237  |            0.0183273 |         0.978315 |   0.873971 |    0.913626 |          0.00339842 |        0.975177 |  0.857072 |   0.905653 | logistic_regression_l2 |    0.177828 |\n",
      "| 12 |   48.6386  |           42.5207    |         0.765363 |   0.28089  |    0.645944 |         13.2126     |        0.765181 |  0.271991 |   0.639759 | svc                    |    0.237137 |\n",
      "| 13 |    3.04386 |            0.0181997 |         0.963199 |   0.794754 |    0.883869 |          0.00579963 |        0.961108 |  0.778219 |   0.873511 | mlp                    |    0.237137 |\n",
      "| 14 |  194.922   |            0.0168003 |         0.97839  |   0.875177 |    0.914601 |          0.00558562 |        0.97459  |  0.850438 |   0.901104 | logistic_regression_l1 |    0.237137 |\n",
      "| 15 |   55.9021  |            0.0166011 |         0.978382 |   0.875139 |    0.914597 |          0.0053987  |        0.974578 |  0.85038  |   0.901098 | logistic_regression_l2 |    0.237137 |\n",
      "| 16 |   42.124   |           37.1679    |         0.748412 |   0.302993 |    0.659065 |         17.2168     |        0.745645 |  0.300422 |   0.656025 | svc                    |    0.316228 |\n",
      "| 17 |    2.50276 |            0.0163994 |         0.951191 |   0.754178 |    0.883606 |          0.0077992  |        0.947687 |  0.735646 |   0.873098 | mlp                    |    0.316228 |\n",
      "| 18 |  174.116   |            0.0153985 |         0.978798 |   0.876769 |    0.91518  |          0.00739942 |        0.974828 |  0.854604 |   0.904037 | logistic_regression_l1 |    0.316228 |\n",
      "| 19 |   50.1296  |            0.0149997 |         0.978815 |   0.876872 |    0.915252 |          0.00699906 |        0.974819 |  0.85456  |   0.904032 | logistic_regression_l2 |    0.316228 |\n",
      "\n",
      "Table with aggregation by: mean, min, max\n",
      "|      |   fit_time |   predict_time_train |   accuracy_train |   f1_train |   auc_train |   predict_time_test |   accuracy_test |   f1_test |   auc_test |   test_size |\n",
      "|:-----|-----------:|---------------------:|-----------------:|-----------:|------------:|--------------------:|----------------:|----------:|-----------:|------------:|\n",
      "| mean |   80.4117  |           11.3653    |         0.912158 |   0.700109 |    0.837036 |             2.69056 |        0.909573 |  0.686175 |   0.829618 |    0.192909 |\n",
      "| min  |    2.47351 |            0.0149997 |         0.705272 |   0.268703 |    0.63566  |             0       |        0.704515 |  0.271991 |   0.639759 |    0.1      |\n",
      "| max  |  230.479   |           51.1095    |         0.978815 |   0.876872 |    0.915252 |            17.2168  |        0.976004 |  0.860075 |   0.906167 |    0.316228 |\n",
      "\n",
      "Pivot table by classifier and test size\n",
      "                                  accuracy_test  auc_test   f1_test\n",
      "clf                    test_size                                   \n",
      "logistic_regression_l1 0.100000        0.975057  0.898300  0.852201\n",
      "                       0.133352        0.976004  0.906167  0.860075\n",
      "                       0.177828        0.975177  0.905650  0.857069\n",
      "                       0.237137        0.974590  0.901104  0.850438\n",
      "                       0.316228        0.974828  0.904037  0.854604\n",
      "logistic_regression_l2 0.100000        0.975057  0.898300  0.852201\n",
      "                       0.133352        0.976004  0.906167  0.860075\n",
      "                       0.177828        0.975177  0.905653  0.857072\n",
      "                       0.237137        0.974578  0.901098  0.850380\n",
      "                       0.316228        0.974819  0.904032  0.854560\n",
      "mlp                    0.100000        0.951257  0.843745  0.731996\n",
      "                       0.133352        0.953037  0.857372  0.735525\n",
      "                       0.177828        0.963094  0.853774  0.777220\n",
      "                       0.237137        0.961108  0.873511  0.778219\n",
      "                       0.316228        0.947687  0.873098  0.735646\n",
      "svc                    0.100000        0.730400  0.673259  0.295967\n",
      "                       0.133352        0.718243  0.651447  0.275323\n",
      "                       0.177828        0.704515  0.639872  0.272510\n",
      "                       0.237137        0.765181  0.639759  0.271991\n",
      "                       0.316228        0.745645  0.656025  0.300422\n",
      "\n",
      "\n",
      "--------------- reuters ---------------\n",
      "\t\tProgress inner loop: 100.00% - 14941.3438s\n",
      "\tProgress test size: 100.00% - 14941.5508s\n",
      "\n",
      "Whole table from experiment:\n",
      "|    |   fit_time |   predict_time_train |   accuracy_train |   f1_train |   auc_train |   predict_time_test |   accuracy_test |   f1_test |   auc_test | clf                    |   test_size |\n",
      "|---:|-----------:|---------------------:|-----------------:|-----------:|------------:|--------------------:|----------------:|----------:|-----------:|:-----------------------|------------:|\n",
      "|  0 |    544.326 |           457.025    |         0.63772  |   0.331509 |    0.74692  |         115.939     |        0.636112 |  0.332409 |   0.74576  | svc                    |    0.202359 |\n",
      "|  1 |    133.415 |             0.122001 |         0.987893 |   0.938938 |    0.955999 |           0.0300295 |        0.981681 |  0.908477 |   0.939688 | mlp                    |    0.202359 |\n",
      "|  2 |  13107.4   |             0.112998 |         0.986008 |   0.929475 |    0.951134 |           0.0280013 |        0.981822 |  0.909558 |   0.941864 | logistic_regression_l1 |    0.202359 |\n",
      "|  3 |    581.958 |             0.112999 |         0.987382 |   0.936497 |    0.955538 |           0.0279992 |        0.98173  |  0.909191 |   0.942051 | logistic_regression_l2 |    0.202359 |\n",
      "\n",
      "Table with aggregation by: mean, min, max\n",
      "|      |   fit_time |   predict_time_train |   accuracy_train |   f1_train |   auc_train |   predict_time_test |   accuracy_test |   f1_test |   auc_test |   test_size |\n",
      "|:-----|-----------:|---------------------:|-----------------:|-----------:|------------:|--------------------:|----------------:|----------:|-----------:|------------:|\n",
      "| mean |   3591.77  |           114.343    |         0.899751 |   0.784105 |    0.902398 |          29.0063    |        0.895336 |  0.764909 |   0.892341 |    0.202359 |\n",
      "| min  |    133.415 |             0.112998 |         0.63772  |   0.331509 |    0.74692  |           0.0279992 |        0.636112 |  0.332409 |   0.74576  |    0.202359 |\n",
      "| max  |  13107.4   |           457.025    |         0.987893 |   0.938938 |    0.955999 |         115.939     |        0.981822 |  0.909558 |   0.942051 |    0.202359 |\n",
      "\n",
      "Pivot table by classifier and test size\n",
      "                                  accuracy_test  auc_test   f1_test\n",
      "clf                    test_size                                   \n",
      "logistic_regression_l1 0.202359        0.981822  0.941864  0.909558\n",
      "logistic_regression_l2 0.202359        0.981730  0.942051  0.909191\n",
      "mlp                    0.202359        0.981681  0.939688  0.908477\n",
      "svc                    0.202359        0.636112  0.745760  0.332409\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with warnings.catch_warnings() and pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        linear_classifiers_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f297d-c37f-4924-bf26-5d4b9dbc10e4",
   "metadata": {},
   "source": [
    "Wnioski:\n",
    "* Dla zbioru 'sonar' najwolniejszy w uczeniu był `MLPClassifier` w każdym przypadku, natomiast dla danych rzadkich był on zdecydowanie najszybszy - znacznie szybszy od pozostałych klasyfikatorów.\n",
    "* Dla danych rzadkich najwolniejsza była `LogisticRegression` wraz funkcją kary `L1`. Gigantyczny czas uczenia w przypadku zbioru danych 'Reuters'. Wcześniejsza wiedza o tym pozwoliła by wykluczyć ten klasyfikator z testów i dokonać eksperyment na większej ilości parametrów.\n",
    "* Porównując dane otrzymane dla zbiorów 'sonar' oraz 'mnist' wynika, że im więcej danych znajdzie się w części uczącej, tym lepsze dokładności dla części testowej zostaną uzyskane.\n",
    "* `SVC` miał najdłuższe czasy predykcji niezależnie od zbioru danych. Im większy zbiór, tym większy czas predykcji.\n",
    "* Powyższy klasyfikator uzyskał najgorsze wyniki (auccuracy_score, f1_score, roc_auc_score) w porównaniu do pozostałych dla zbiorów danych 'Reuters' oraz 'mnist'. Wyniki te znacznie odstają od pozostałych. \n",
    "* Brak zastosowania własnej implementacji `SVM2` wynika z niezidentyfikowanych błędów związanych z biblioteką `cvxopt`. Natomiast brak `LogisticRegression` z funkcją kary `elasticnet` wynika z niedokładnego przeczytania dokumentacji oraz niezastosowaniu parametru `l1_ratio=0.5`, którego brak powodował błędy w działaniu programu.\n",
    "* Podsumowanie zbiorów:\n",
    "    * 'sonar'\n",
    "        * `MLPClassifier` najwolniejszy czas uczenia, \n",
    "        * `SVC` najlepsze wyniki zarówno na części uczącej jak i testowej\n",
    "    * 'mnist' - \n",
    "        * `LogisticRegression` z funkcją kary `L1` - najwolniejszy czas uczenia, \n",
    "        * `SVC` najdłuższy czas predykcji i odstające dokładności klasyfikacji, \n",
    "        * `MLPClassifier` najszybszy jeżeli chodzi o czas uczenia, \n",
    "        * `LogisticRegression` niezależnie od funkcji kary daje najlepsze dokładności klasyfikacji, zarówno na części uczącej jak i na cześci testowej\n",
    "    * 'Reuters' - \n",
    "        * `LogisticRegression` z funkcją kary `L1` - gigantyczny czas uczenia,\n",
    "        * `SVC` najdłuższy czas predykcji i odstające dokładności klasyfikacji,\n",
    "        * `MLPClassifier` najszybszy jeżeli chodzi o czas uczenia,\n",
    "        * `SVC` jako jedyny odstaje dokładnościami klasyfikacji od konkurencyjnych klasyfikatorów, które dają porównywalne, bliskie siebie wartości jako dokładności klasyfikacji"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
