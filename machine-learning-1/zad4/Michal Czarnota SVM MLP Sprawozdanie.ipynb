{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d1f32a",
   "metadata": {},
   "source": [
    "Celem laboratorium było wykonanie testów porównujących działanie oraz wydajność implementacji własnych wraz z implementacjami z bibliotek dla SVM oraz MLP.\n",
    "SVM miało być porównane jako własna implementacja z regresją liniową jako pakietem z biblioteki `scikit-learn`.\n",
    "Własne implementacje MLP jako backpropagation oraz MlpExtreme również miały być se sobą porównane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aec3c3",
   "metadata": {},
   "source": [
    "Importy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5879328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import cvxopt as cvx\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f0fb64",
   "metadata": {},
   "source": [
    "Poniżej implementacja klasy abstrakcyjnej będącej bazą dla pozostałych klasyfikatorów.\n",
    "\n",
    "Opis niepodpisanych parametrów:\n",
    "* `class_labels_` - unikalne nazwy klas\n",
    "* `max_seconds` - właściwość określający po jakim czasie uczenie powinno zostać przerwane. Wykorzystywane szczególnie w przypadku implementacji funkcji `fit` w perceptronach\n",
    "* `iteration_count` - właściwość obiektu służąca do przechowywania liczby wykonanych iteracji w trakcie uczenia. Używane szczególnie w przypadku implementacji funkcji `fit` dla perceptronów\n",
    "\n",
    "Opis metod:\n",
    "* `fit` - abstrakcyjna metoda przyjmująca dane jako `x` oraz decyzje jako `d`\n",
    "* `margin` - metoda wyznaczająca margines separacji pomiędzy nauczonymi danymi\n",
    "* `plot_class` - metoda wizualizująca podział danych z wykorzystaniem albo linii separucjącej albo konturów\n",
    "* `plot_class_universal` - statyczna wersja metody `plot_class`, która przyjmuje dowolny klasyfikator spełniający założenia szablonu klasyfikatorów liniowych z biblioteki `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1583e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(ABC):\n",
    "    def __init__(self, coefs=None, intercepts=None, class_labels=None, max_seconds: int = 3600):\n",
    "        self.coef_ = coefs  # w\n",
    "        self.intercept_ = intercepts  # b\n",
    "        self.class_labels_ = class_labels\n",
    "\n",
    "        self.max_seconds = max_seconds\n",
    "        self.iteration_count = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, x, d):\n",
    "        ...\n",
    "\n",
    "    def margin(self, x, d, distance: bool = False):\n",
    "        # margines w danych to jest minimum z wektora. jak margines jest dodatni, to znaczy, że jest separowalne\n",
    "        margin = (x.dot(self.coef_) + self.intercept_) * d\n",
    "        if distance:\n",
    "            margin /= np.linalg.norm(self.coef_)\n",
    "\n",
    "        return margin\n",
    "\n",
    "    def decision_function(self, x: np.array):\n",
    "        return self.margin(x, np.ones(x.shape[0]))\n",
    "\n",
    "    def predict_proba(self, x: np.array):\n",
    "        a, b = self.margin(x, np.ones((x.shape[0], )))\n",
    "        i = 1 - 1 / (1 + np.exp(-b))\n",
    "        j = 1 / (1 + np.exp(-b))\n",
    "\n",
    "        return np.array([i, j]).T\n",
    "\n",
    "    def predict(self, x: np.array):\n",
    "        results = np.sign(x.dot(self.coef_) + self.intercept_)\n",
    "        results_mapped = self.class_labels_[1 * (results > 0)] if self.class_labels_ is not None else results\n",
    "\n",
    "        return results_mapped\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        pass\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.__class__.__name__}'\n",
    "\n",
    "    def plot_class(self, x, y, is_line: bool = False, support_vectors: np.array = None, dataset_title: str = None):\n",
    "        self.plot_class_universal(self, x, y, is_line, support_vectors, dataset_title)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_class_universal(clf, x, y, is_line: bool = False, support_vectors: np.array = None, dataset_title: str = None):\n",
    "        x1_min = np.min(x[:, 0]) - 0.5\n",
    "        x1_max = np.max(x[:, 0]) + 0.5\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "\n",
    "        if is_line:\n",
    "            points = np.array([[i, -(clf.coef_[0] * i + clf.intercept_) / clf.coef_[1]] for i in np.linspace(x1_min, x1_max)])\n",
    "            plt.plot(points[:, 0], points[:, 1], 'k')\n",
    "        else:\n",
    "            x2_min = np.min(x[:, 1]) - 0.5\n",
    "            x2_max = np.max(x[:, 1]) + 0.5\n",
    "\n",
    "            number_of_points = 250\n",
    "            xn, yn = np.meshgrid(np.linspace(x1_min, x1_max, number_of_points), np.linspace(x2_min, x2_max, number_of_points))\n",
    "            zn = clf.predict(np.c_[xn.flatten(), yn.flatten()]).reshape(xn.shape)\n",
    "\n",
    "            plt.contourf(xn, yn, zn, cmap=ListedColormap(['y', 'r']))\n",
    "\n",
    "        if support_vectors is not None:\n",
    "            plt.plot(x[support_vectors, 0], x[support_vectors, 1], 'co', markersize=10, alpha=0.5)\n",
    "\n",
    "        plt.scatter(x[:, 0], x[:, 1], c=y, cmap=ListedColormap(['b', 'g']))\n",
    "\n",
    "        dataset_title = f'for {dataset_title} dataset' if dataset_title else ''\n",
    "        plt.title(f'{clf} - Boundary of separation {dataset_title}')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416437b2",
   "metadata": {},
   "source": [
    "Poniżej reprezentacja klasy implementującej SVM.\n",
    "\n",
    "Opis parametrów:\n",
    "* `c_` - parametr regularyzacyjny\n",
    "* `sv_indexes_` - indeksy próbek podpierających"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773f669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Svm2(LinearClassifier):\n",
    "    def __init__(self, c: float = 1.0, cls_lab=None):\n",
    "        super().__init__(cls_lab)\n",
    "\n",
    "        self.c_ = c\n",
    "        self.sv_indexes_ = None\n",
    "\n",
    "    def fit(self, x: np.array, y: np.array):\n",
    "        m, n = x.shape\n",
    "\n",
    "        gp = -np.hstack((np.outer(y, np.ones(n + 1)) * np.hstack((x, np.ones((m, 1)))), np.eye(m)))\n",
    "        gpp = np.hstack((np.zeros((m, n + 1)), -np.eye(m)))\n",
    "        g = cvx.matrix(np.vstack((gp, gpp)))\n",
    "\n",
    "        h = cvx.matrix(np.concatenate((np.ones(m) * -1, np.zeros(m))))\n",
    "        p = cvx.matrix(np.diag(np.concatenate((np.ones(n), np.zeros(m + 1)))))\n",
    "        q = cvx.matrix(np.concatenate((np.zeros(n + 1), self.c_ * np.ones(m))))\n",
    "\n",
    "        solution = cvx.solvers.qp(p, q, g, h)\n",
    "\n",
    "        self.coef_ = solution['x'][:n]\n",
    "        self.intercept_ = solution['x'][n]\n",
    "        self.sv_indexes_ = np.nonzero(np.array(solution['z'][:n]) > 10e-5)[0]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'SVM2(c={self.c_})'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64e095",
   "metadata": {},
   "source": [
    "Poniżej reprezantacja implementacji klasy do wykonywania eksperymentów związanych z SVM.\n",
    "Testy przedstawiają działanie własnej implementacji SVM poprzez klasę `SVM2`, implementacji SVM z biblioteki `scikit-learn` oraz implementacji regresji liniowej również z biblioteki `scikit-learn`.\n",
    "Główna metoda `experiment` jest odpowiedzialna za uruchomienie eksperymentów dla wszystkich implementacji w wykorzystaniem liniowo separowalnego, liniowo nieseparowalnego zbioru danych oraz zbioru danych 'sonar'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "258eabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SvmTest:\n",
    "    def __init__(self, c: float = 1e-3):\n",
    "        self.c_ = c\n",
    "\n",
    "    def experiment(self):\n",
    "        sonar_data = pd.read_csv('sonar_csv.csv')\n",
    "        sonar_y = self.normalize_decisions(sonar_data[sonar_data.columns[-1]])\n",
    "        sonar_x = sonar_data.drop(sonar_data.columns[-1], axis=1).to_numpy()\n",
    "\n",
    "        datasets = {\n",
    "            # 'linear_separable': self.generate_linear_separable_dataset(),\n",
    "            'linear_non_separable': self.generate_linear_non_separable_dataset(),\n",
    "            # 'sonar': [sonar_x, sonar_y],\n",
    "        }\n",
    "\n",
    "        for title, dataset in datasets.items():\n",
    "            print(f'\\n------ Dataset {title} ------')\n",
    "            x, y = dataset\n",
    "\n",
    "            for method in [self.svm_my_experiment, self.svm_sklearn_experiment, self.linear_regression_sklearn_experiment]:\n",
    "                method(x, y, title)\n",
    "\n",
    "    def svm_my_experiment(self, x: np.array, y: np.array, dataset_title: str):\n",
    "        svm2 = Svm2(c=self.c_)\n",
    "\n",
    "        self.fit_measure(svm2, x, y)\n",
    "        self.predict_measure_and_visualise(svm2, x, y, dataset_title)\n",
    "\n",
    "        print(f'Separation margin for {svm2}: {svm2.margin(x, y)}')\n",
    "        print(f'Support vectors for {svm2}\\n{svm2.sv_indexes_}')\n",
    "\n",
    "    def svm_sklearn_experiment(self, x: np.array, y: np.array, dataset_title: str):\n",
    "        svc = SVC(C=self.c_)\n",
    "        self.fit_measure(svc, x, y)\n",
    "        self.predict_measure_and_visualise(svc, x, y, dataset_title)\n",
    "\n",
    "        print(f'Support vectors for {svc}\\n{svc.support_vectors_}')\n",
    "\n",
    "    def linear_regression_sklearn_experiment(self, x: np.array, y: np.array, dataset_title: str):\n",
    "        linear_regression = LinearRegression()\n",
    "        self.fit_measure(linear_regression, x, y)\n",
    "        self.predict_measure_and_visualise(linear_regression, x, y, dataset_title)\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_measure(clf, x, y):\n",
    "        t1 = time.time()\n",
    "        clf.fit(x, y)\n",
    "        t2 = time.time()\n",
    "        print(f'Time of fitting for {clf}: {t2 - t1}s')\n",
    "\n",
    "    @staticmethod\n",
    "    def predict_measure_and_visualise(clf, x, y, dataset_title: str):\n",
    "        t1 = time.time()\n",
    "        LinearClassifier.plot_class_universal(clf, x, y, dataset_title=dataset_title)\n",
    "        t2 = time.time()\n",
    "        print(f'Time of plotting with predicting for {clf}: {t2 - t1}s')\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_simple_separable_dataset():\n",
    "        x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "        y = np.array([-1, -1, -1, 1])\n",
    "\n",
    "        return [x, y]\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_linear_separable_dataset(n: int = 1000):\n",
    "        w, b = [1, 1], -1\n",
    "        x = np.random.randn(100, 2)\n",
    "        d = np.sign(x.dot(w) + b)\n",
    "\n",
    "        return [x, d]\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_linear_non_separable_dataset(n: int = 1000):\n",
    "        x = np.vstack((np.random.randn(n, 2), np.random.randn(n, 2) + 2))\n",
    "        y = np.concatenate((np.ones(n), -np.ones(n)))\n",
    "\n",
    "        return [x, y]\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_decisions(d):\n",
    "        d_normalized = np.ones(d.shape[0]).astype(\"int8\")\n",
    "        d_normalized[d == np.unique(d)[0]] = -1\n",
    "\n",
    "        return d_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd8925",
   "metadata": {},
   "source": [
    "Poniżej klasa reprezentująca implementację wersji MLP jako Extreme.\n",
    "Implementacja zakłada jedną warstę ukrytą.\n",
    "\n",
    "Opis parametrów:\n",
    "* `clf_lin` - dowolny klasyfikator liniowy używany do uczenia oraz predykcji\n",
    "* `neurons_hidden_count` - liczba neuronów w warstwie ukrytej\n",
    "* `w1` - wagi warstwy wejściowej\n",
    "* `b1` - wartości odchylenia warsty wejściowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4249d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpExtreme(LinearClassifier):\n",
    "    def __init__(self, clf_lin, neurons_hidden_count: int = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.clf_lin = clf_lin  # można wstawić za klasyfikator liniowy SVM\n",
    "        self.neurons_hidden_count = neurons_hidden_count\n",
    "\n",
    "        self.w1 = None\n",
    "        self.b1 = None\n",
    "\n",
    "    def fit(self, x: np.array, d: np.array):\n",
    "        self.w1 = np.zeros((x.shape[1], self.neurons_hidden_count))  # wagi warstry wejściowej, wyznaczyć losując\n",
    "        self.b1 = np.zeros(self.neurons_hidden_count)  # wartości odchylenia warstry wejściowej, wyznaczyć losując\n",
    "\n",
    "        for i in range(self.neurons_hidden_count):\n",
    "            i1, i2 = np.random.choice(x.shape[0], 2)\n",
    "            xi, xj = x[i1, :], x[i2, :]\n",
    "\n",
    "            self.w1[:, i] = np.transpose(xj - xi)\n",
    "            self.b1[i] = -self.w1[:, i].dot(xi)  # chcemy dodać b do każdej kolumny\n",
    "\n",
    "        v = self.sigmoid(x.dot(self.w1) + self.b1)\n",
    "        # v = self.sigmoid(self.w1.T.dot(x) + self.b1)\n",
    "        self.clf_lin.fit(v, d)  # dowolny klasyfikator liniowy posiadający fit\n",
    "\n",
    "        self.coef_ = self.clf_lin.coef_\n",
    "        self.intercept_ = self.clf_lin.intercept_\n",
    "\n",
    "    def predict(self, x: np.array):\n",
    "        v = self.sigmoid(x.dot(self.w1) + self.b1)\n",
    "\n",
    "        return self.clf_lin.predict(v)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173ec07",
   "metadata": {},
   "source": [
    "Poniżej klasa reprezentująca implementację back propagation MLP z jedną warstwą ukrytą.\n",
    "\n",
    "Opis parametrów:\n",
    "* `neurons_hidden_count` - liczba neuronów w warstwie ukrytej\n",
    "* `max_iter` - maksymalna liczba iteracji wykonywania algorytmu dopasowania (uczenia)\n",
    "* `alpha` - parametr regularyzacyjny\n",
    "* `shuffle` - parametr określający czy mieszać próbki uczące co iterację. Mieszanie próbek uczących przyspiesza uczenie\n",
    "* `weights_scale` - mnożnik dla wag, który zmniejsza ich wartości do bardzo małych\n",
    "* `hidden_weights` - wektor wag pomiędzy warstwą wejściową a warstwą ukrytą\n",
    "* `hidden_biases` - wartości odchyleń pomiędzy warstwą wejściową a warstwą ukrytą\n",
    "\n",
    "Opis metod: \n",
    "* `fit` - metoda dopasowująca model do próbek uczących. Przygotowuje ona wektory wag oraz wektory wartości odchyleń oraz wykonuje iteracje aż do `max_iter`.\n",
    "* `forward_propagation` - wykonuje propagację wprzód, krok do przodu. Wylicza wartości aktywacji dla warstw ukrytej oraz wyjściowej\n",
    "* `back_propagation` - wykonuje krok w tył, wsteczna propagacja. Wylicza delty oraz gradienty jako wartości poprawek dla warstw ukrytej oraz wyjściowej\n",
    "* `update_weights` - wykonuje poprawki wag oraz wartości odchyleń\n",
    "* `sigmoid` - funkcja aktywacji\n",
    "* `normalize_decisions` - normalizuje decyzje, aby reprezentowane one były przez dwie klasy: -1 oraz 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4631a40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpBackPropagation(LinearClassifier):\n",
    "    def __init__(self, neurons_hidden_count: int = 100, max_iter: int = 1000, alpha: float = 0.01, shuffle: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.neurons_hidden_count = neurons_hidden_count\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.weights_scale = 10 ** -3\n",
    "\n",
    "        self.hidden_weights = None\n",
    "        self.hidden_biases = None\n",
    "\n",
    "    def fit(self, x: np.array, d: np.array):\n",
    "        y = self.normalize_decisions(d, x)\n",
    "        self.class_labels_ = np.unique(y)\n",
    "\n",
    "        self.hidden_weights = np.random.normal(size=(x.shape[1], self.neurons_hidden_count)) * self.weights_scale\n",
    "        self.hidden_biases = np.random.normal(size=self.neurons_hidden_count)\n",
    "\n",
    "        self.coef_ = np.random.normal(size=(self.neurons_hidden_count, y.shape[1])) * self.weights_scale\n",
    "        self.intercept_ = np.random.normal(size=y.shape[1])\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(x)\n",
    "\n",
    "            for x_iter, x_val in enumerate(x):\n",
    "                params_activations = self.forward_propagation(x_val)\n",
    "                params_fixes = self.back_propagation(x_val, y[x_iter], params_activations)\n",
    "                self.update_weights(params_fixes)\n",
    "\n",
    "    def forward_propagation(self, x: np.array):\n",
    "        z1 = x.dot(self.hidden_weights) + self.hidden_biases  # neuron value at hidden layer\n",
    "        a1 = self.sigmoid(z1)  # activation value at output layer\n",
    "        z2 = a1.dot(self.coef_) + self.intercept_  # neuron value at output layer\n",
    "        a2 = self.sigmoid(z2)  # activation value at output layer\n",
    "\n",
    "        return {'a1': a1, 'a2': a2}\n",
    "\n",
    "    def back_propagation(self, x: np.array, d: np.array, params: dict):\n",
    "        delta_out = (params['a2'] - d) * (params['a2'] * (1 - params['a2']))\n",
    "        gradient_out = np.outer(params['a1'], delta_out)\n",
    "\n",
    "        delta_hidden = np.dot(delta_out, self.coef_.T) * (params['a1'] * (1 - params['a1']))\n",
    "        gradient_hidden = np.outer(x, delta_hidden)\n",
    "\n",
    "        return {'delta_out': delta_out, 'gradient_out': gradient_out, 'delta_hidden': delta_hidden, 'gradient_hidden': gradient_hidden}\n",
    "\n",
    "    def update_weights(self, params: dict):\n",
    "        self.hidden_weights -= self.alpha * params['gradient_hidden']\n",
    "        self.hidden_biases -= self.alpha * params['delta_hidden']\n",
    "        self.coef_ -= self.alpha * params['gradient_out']\n",
    "        self.intercept_ -= self.alpha * params['delta_out']\n",
    "\n",
    "    def predict(self, x: np.array):\n",
    "        a2 = self.forward_propagation(x)['a2']\n",
    "        if len(a2.shape) == 1:\n",
    "            return self.class_labels_[np.argmax(a2)]\n",
    "\n",
    "        return np.array([self.class_labels_[np.argmax(pair)] for pair in a2])\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_decisions(d: np.array, x: np.array):\n",
    "        classes = np.unique(d)\n",
    "        y = np.full((x.shape[0], classes.shape[0]), 1)\n",
    "\n",
    "        for i in range(x.shape[0]):\n",
    "            y[i, np.where(classes == d[i])[0]] = 1\n",
    "\n",
    "        return y\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'MlpBackPropagation(neurons_hidden_count={self.neurons_hidden_count}, max_iter={self.max_iter}, alpha={self.alpha}, shuffle={self.shuffle})'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c1893",
   "metadata": {},
   "source": [
    "Poniżej reprezantacja implementacji klasy do wykonywania eksperymentów związanych z MLP.\n",
    "Testy przedstawiają działanie własnej implementacji dla MLP BackPropagation, MLP Extreme oraz dla implemetacji `MlpClassifier` istniejącej w bibliotece `scikit-learn`.\n",
    "Wersja MLP Extreme wykorzystuje klasyfikator liniowy SVM jako własną implementację `SVM2`.\n",
    "\n",
    "Opis parametrów:\n",
    "* `layers` - liczba warstw ukrytych\n",
    "* `neurons_count_in_one_hidden_layer` - liczba neuronów w jednej warstwie ukrytej\n",
    "* `train_data_size` - rozmiar danych uczących względem całego zbioru danych\n",
    "* `alpha` - parametr regularyzacyjny\n",
    "\n",
    "Główna metoda `experiment` uruchamia eksperymenty dla wszystkich wymienionych wyżej klasyfikatorach stosując kombinację wszystkich permutacji opisanych powyżej parametrów.\n",
    "Zbiory danych generowane na potrzeby eksperymentów to szachownica, zagnieżdżone okręgi oraz zagnieżdżone spirale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c17ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpTest:\n",
    "    def __init__(self):\n",
    "        self.layers = [1]\n",
    "        self.neurons_count_in_one_hidden_layer = [10, 30, 50, 100, 300, 1000]\n",
    "        self.train_data_size = [0.5, 0.6, 0.8]\n",
    "        self.alpha = [10 ** i for i in range(-5, 3)]\n",
    "\n",
    "    def experiment(self):\n",
    "        datasets_n = [1000, 10000, 100000]\n",
    "\n",
    "        for n in datasets_n:\n",
    "            experiment_datasets = {\n",
    "                'chessboard': self.generate_chessboard_dataset(n),\n",
    "                'circles': self.generate_circles_dataset(n),\n",
    "                'spirals': self.generate_spirals_dataset(n),\n",
    "            }\n",
    "\n",
    "            for title, (x, y) in experiment_datasets.items():\n",
    "                self.dataset_experiment((f'{title}_n_{n}', x, y))\n",
    "\n",
    "    def dataset_experiment(self, dataset: tuple):\n",
    "        for layers in self.layers:\n",
    "            for neurons_count_in_one_hidden_layer in self.neurons_count_in_one_hidden_layer:\n",
    "                for train_data_size in self.train_data_size:\n",
    "                    neurons = tuple([neurons_count_in_one_hidden_layer for _ in range(layers)])\n",
    "\n",
    "                    for alpha in self.alpha:\n",
    "                        self.mlp_experiment(neurons, train_data_size, alpha, dataset)\n",
    "\n",
    "    def mlp_experiment(self, neurons_in_hidden_layers: tuple, train_data_size: float, alpha: float, dataset: tuple):\n",
    "        dataset_title, x, y = dataset\n",
    "        dataset_title += f'_train_size_{train_data_size}'\n",
    "        divided_train_test = train_test_split(x, y, train_size=train_data_size)\n",
    "\n",
    "        for method in [self.mlp_sklearn_experiment, self.mlp_backprop_experiment, self.mlp_extreme_experiment]:\n",
    "            print('')\n",
    "            method(neurons_in_hidden_layers, divided_train_test, alpha, dataset_title)\n",
    "\n",
    "    def mlp_sklearn_experiment(self, neurons_in_hidden_layers, divided_train_test: tuple, alpha: float, dataset_title: str):\n",
    "        mlp = MLPClassifier(neurons_in_hidden_layers, alpha=alpha, max_iter=1000)\n",
    "        self.experiment_for_specific_mlp(mlp, divided_train_test, dataset_title)\n",
    "\n",
    "    def mlp_backprop_experiment(self, neurons_in_hidden_layers: tuple, divided_train_test: tuple, alpha: float, dataset_title: str):\n",
    "        mlp = MlpBackPropagation(neurons_in_hidden_layers[0], max_iter=1000, alpha=alpha)\n",
    "        self.experiment_for_specific_mlp(mlp, divided_train_test, dataset_title)\n",
    "        \n",
    "    def mlp_extreme_experiment(self, neurons_in_hidden_layers: tuple, divided_train_test: tuple, alpha: float, dataset_title: str):\n",
    "        mlp = MlpExtreme(Svm2(), neurons_in_hidden_layers[0])\n",
    "        self.experiment_for_specific_mlp(mlp, divided_train_test, dataset_title)\n",
    "\n",
    "    def experiment_for_specific_mlp(self, mlp, divided_train_test: tuple, dataset_title: str):\n",
    "        x_train, x_test, y_train, y_test = divided_train_test\n",
    "\n",
    "        t1 = time.time()\n",
    "        mlp.fit(x_train, y_train)\n",
    "        t2 = time.time()\n",
    "        print(f'Time of fitting on {dataset_title} for {mlp}: {t2 - t1}s')\n",
    "\n",
    "        self.classification_quality(mlp, x_train, y_train, 'train')\n",
    "        self.classification_quality(mlp, x_test, y_test, 'test')\n",
    "\n",
    "        LinearClassifier.plot_class_universal(mlp, x_test, y_test, dataset_title=dataset_title)\n",
    "\n",
    "    @staticmethod\n",
    "    def classification_quality(mlp, x: np.array, y: np.array, classification_type: str = 'test'):\n",
    "        t1 = time.time()\n",
    "        y_pred = mlp.predict(x)\n",
    "        t2 = time.time()\n",
    "        print(f'Time of {mlp} prediction for {classification_type} data: {t2 - t1}s')\n",
    "\n",
    "        print(f'Metrics of {mlp} for {classification_type}:')\n",
    "        accuracy = metrics.accuracy_score(y, y_pred)\n",
    "        print(f'\\tAccuracy: {accuracy}')\n",
    "\n",
    "        f1_score = metrics.f1_score(y, y_pred)\n",
    "        print(f'\\tF1: {f1_score}')\n",
    "\n",
    "        auc_score = metrics.roc_auc_score(y, y_pred)\n",
    "        print(f'\\tAUC: {auc_score}')\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_chessboard_dataset(n: int = 1000, m: int = 3):\n",
    "        x = np.random.rand(n, 2) * m\n",
    "        y = np.mod(np.sum(np.floor(x), axis=1), 2) * 2. - 1.\n",
    "        x = x + np.random.randn(*x.shape) * 0.1\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_circles_dataset(n: int = 1000):\n",
    "        x, y = datasets.make_circles(n)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_spirals_dataset(n: int = 100, noise: float = 0.1, length: int = 2):\n",
    "        t = np.linspace(0, (2 * np.pi * length) ** 2, n // 2)\n",
    "        t = t ** 0.5\n",
    "\n",
    "        x1 = (0.2 + t) * np.cos(t)\n",
    "        y1 = (0.2 + t) * np.sin(t)\n",
    "        x2 = (0.2 + t) * np.cos(t + np.pi)\n",
    "        y2 = (0.2 + t) * np.sin(t + np.pi)\n",
    "\n",
    "        x = np.array([np.concatenate((x1, x2)), np.concatenate((y1, y2))]).T + np.random.randn(n, 2) * noise\n",
    "        y = np.concatenate((-np.ones(n // 2), +np.ones(n // 2)))\n",
    "        p = np.random.permutation(n)\n",
    "\n",
    "        x = x[p, :]\n",
    "        y = y[p]\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a72351",
   "metadata": {},
   "source": [
    "Wykonanie eksperymentów dla części związanej z SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c507e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_test = SvmTest()\n",
    "svm_test.experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963bae09",
   "metadata": {},
   "source": [
    "Wykonanie eksperymentów dla części związanej z MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpTest = MlpTest()\n",
    "mlpTest.experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b939853",
   "metadata": {},
   "source": [
    "Zestawienie wyników:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071638f3",
   "metadata": {},
   "source": [
    "Wnioski:\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f6846a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
